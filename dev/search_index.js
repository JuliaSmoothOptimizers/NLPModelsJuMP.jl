var documenterSearchIndex = {"docs":
[{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"​","category":"section"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"​\n\nPages = [\"reference.md\"]\n\n​","category":"section"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"​\n\nPages = [\"reference.md\"]\n\n​","category":"section"},{"location":"reference/#NLPModelsJuMP.MathOptNLPModel-Tuple{JuMP.Model}","page":"Reference","title":"NLPModelsJuMP.MathOptNLPModel","text":"MathOptNLPModel(model, hessian=true, name=\"Generic\")\n\nConstruct a MathOptNLPModel from a JuMP model.\n\nhessian should be set to false for multivariate user-defined functions registered without hessian.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsJuMP.MathOptNLSModel-Tuple{JuMP.Model, Any}","page":"Reference","title":"NLPModelsJuMP.MathOptNLSModel","text":"MathOptNLSModel(model, F, hessian=true, name=\"Generic\")\n\nConstruct a MathOptNLSModel from a JuMP model and a container of JuMP GenericAffExpr (generated by @expression) and NonlinearExpression (generated by @NLexpression).\n\nhessian should be set to false for multivariate user-defined functions registered without hessian.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsJuMP.NonLinearStructure","page":"Reference","title":"NLPModelsJuMP.NonLinearStructure","text":"NonLinearStructure\n\nStructure containing Jacobian and Hessian structures of nonlinear constraints:\n\nnnln: number of nonlinear constraints\nnl_lcon: lower bounds of nonlinear constraints\nnl_ucon: upper bounds of nonlinear constraints\njac_rows: row indices of the Jacobian in Coordinate format (COO) format\njac_cols: column indices of the Jacobian in COO format\nnnzj: number of non-zero entries in the Jacobian\nhess_rows: row indices of the Hessian in COO format\nhess_cols: column indices of the Hessian in COO format\nnnzh: number of non-zero entries in the Hessian\n\n\n\n\n\n","category":"type"},{"location":"reference/#NLPModelsJuMP.Oracles","page":"Reference","title":"NLPModelsJuMP.Oracles","text":"Oracles\n\nStructure containing nonlinear oracles data:\n\noracles: vector of tuples (MOI.VectorOfVariables, _VectorNonlinearOracleCache)\nncon: number of scalar constraints represented by all oracles\nlcon: lower bounds of oracle constraints\nucon: upper bounds of oracle constraints\nnnzj: number of non-zero entries in the Jacobian of all oracles\nnnzh: number of non-zero entries in the Hessian of all oracles\nnzJ: buffer to store the nonzeros of the Jacobian for all oracles (needed for the functions jprod and jtprod)\nnzH: buffer to store the nonzeros of the Hessian for all oracles (needed for the function hprod)\nhessianoraclessupported: support of the Hessian for all oracles\n\n\n\n\n\n","category":"type"},{"location":"reference/#NLPModelsJuMP.add_constraint_model-Tuple{Any, JuMP.NonlinearExpression}","page":"Reference","title":"NLPModelsJuMP.add_constraint_model","text":"add_constraint_model(Fmodel, Fi)\n\nAdd the nonlinear constraint Fi == 0 to the model Fmodel. If Fi is an Array, then we iterate over each component.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsJuMP.coo_sym_add_mul!-Tuple{AbstractVector{<:Integer}, AbstractVector{<:Integer}, AbstractVector, AbstractVector, AbstractVector, Float64}","page":"Reference","title":"NLPModelsJuMP.coo_sym_add_mul!","text":"coo_sym_add_mul!(rows, cols, vals, x, y, α)\n\nPerform the update y ← y + α * A * x where A is a symmetric matrix in COO format given by (rows, cols, vals). Only one triangle of A should be passed.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsJuMP.coo_sym_dot-Tuple{AbstractVector{<:Integer}, AbstractVector{<:Integer}, AbstractVector, AbstractVector, AbstractVector}","page":"Reference","title":"NLPModelsJuMP.coo_sym_dot","text":"coo_sym_dot(rows, cols, vals, x, y)\n\nCompute the product xᵀAy of a symmetric matrix A given by (rows, cols, vals). Only one triangle of A should be passed.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsJuMP.coo_unsym_add_mul!-Tuple{Bool, AbstractVector{<:Integer}, AbstractVector{<:Integer}, AbstractVector, AbstractVector, AbstractVector, Float64}","page":"Reference","title":"NLPModelsJuMP.coo_unsym_add_mul!","text":"coo_unsym_add_mul!(transpose, rows, cols, vals, x, y, α)\n\nPerforms the update y ← y + α * op(A) * x, where A is an unsymmetric matrix in COO format given by (rows, cols, vals). If transpose == true, then op(A) = Aᵀ; otherwise, op(A) = A.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsJuMP.parser_MOI-Tuple{Any, Any, Any}","page":"Reference","title":"NLPModelsJuMP.parser_MOI","text":"parser_MOI(moimodel, index_map, nvar)\n\nParse linear constraints of a MOI.ModelLike.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsJuMP.parser_NL-Tuple{Any}","page":"Reference","title":"NLPModelsJuMP.parser_NL","text":"parser_NL(nlp_data; hessian)\n\nParse nonlinear constraints of an nlp_data.\n\nReturns:\n\nnlcon: NonLinearStructure containing Jacobian and Hessian structures\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsJuMP.parser_SAF-NTuple{9, Any}","page":"Reference","title":"NLPModelsJuMP.parser_SAF","text":"parser_SAF(fun, set, linrows, lincols, linvals, nlin, lin_lcon, lin_ucon, index_map)\n\nParse a ScalarAffineFunction fun with its associated set. linrows, lincols, linvals, lin_lcon and lin_ucon are updated.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsJuMP.parser_SQF-NTuple{7, Any}","page":"Reference","title":"NLPModelsJuMP.parser_SQF","text":"parser_SQF(fun, set, nvar, qcons, quad_lcon, quad_ucon, index_map)\n\nParse a ScalarQuadraticFunction fun with its associated set. qcons, quad_lcon, quad_ucon are updated.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsJuMP.parser_VAF-NTuple{9, Any}","page":"Reference","title":"NLPModelsJuMP.parser_VAF","text":"parser_VAF(fun, set, linrows, lincols, linvals, nlin, lin_lcon, lin_ucon, index_map)\n\nParse a VectorAffineFunction fun with its associated set. linrows, lincols, linvals, lin_lcon and lin_ucon are updated.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsJuMP.parser_VQF-NTuple{7, Any}","page":"Reference","title":"NLPModelsJuMP.parser_VQF","text":"parser_VQF(fun, set, nvar, qcons, quad_lcon, quad_ucon, index_map)\n\nParse a VectorQuadraticFunction fun with its associated set. qcons, quad_lcon, quad_ucon are updated.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsJuMP.parser_linear_expression-NTuple{4, Any}","page":"Reference","title":"NLPModelsJuMP.parser_linear_expression","text":"parser_linear_expression(cmodel, nvar, index_map, F)\n\nParse linear expressions of type VariableRef and GenericAffExpr{Float64,VariableRef}.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsJuMP.parser_nonlinear_expression-Tuple{Any, Any, Any}","page":"Reference","title":"NLPModelsJuMP.parser_nonlinear_expression","text":"parser_nonlinear_expression(cmodel, nvar, F; hessian)\n\nParse nonlinear expressions of type NonlinearExpression.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsJuMP.parser_objective_MOI-Tuple{Any, Any, Any}","page":"Reference","title":"NLPModelsJuMP.parser_objective_MOI","text":"parser_objective_MOI(moimodel, nvar, index_map)\n\nParse linear and quadratic objective of a MOI.ModelLike.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsJuMP.parser_oracles-Tuple{Any}","page":"Reference","title":"NLPModelsJuMP.parser_oracles","text":"oracles = parser_oracles(moimodel)\n\nParse nonlinear oracles of a MOI.ModelLike.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsJuMP.parser_variables-Tuple{MathOptInterface.ModelLike}","page":"Reference","title":"NLPModelsJuMP.parser_variables","text":"parser_variables(model)\n\nParse variables informations of a MOI.ModelLike.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsJuMP.replace!-Tuple{Any, Any}","page":"Reference","title":"NLPModelsJuMP.replace!","text":"replace!(ex, x)\n\nWalk the expression ex and substitute in the actual variables x.\n\n\n\n\n\n","category":"method"},{"location":"#NLPModelsJuMP.jl-documentation","page":"Home","title":"NLPModelsJuMP.jl documentation","text":"This package defines a NLPModels model using MathOptInterface and JuMP.jl. This documentation is specific for this model. Please refer to the NLPModels documentation if in doubt.","category":"section"},{"location":"#Install","page":"Home","title":"Install","text":"Install NLPModelsJuMP.jl with the following commands.\n\npkg> add NLPModelsJuMP","category":"section"},{"location":"#Bug-reports-and-discussions","page":"Home","title":"Bug reports and discussions","text":"If you think you found a bug, feel free to open an issue. Focused suggestions and requests can also be opened as issues. Before opening a pull request, start an issue or a discussion on the topic, please.\n\nIf you want to ask a question not suited for a bug report, feel free to start a discussion here. This forum is for general discussion about this repository and the JuliaSmoothOptimizers organization, so questions about any of our packages are welcome.","category":"section"},{"location":"#Contents","page":"Home","title":"Contents","text":"","category":"section"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"NLPModelsJuMP is a combination of NLPModels and JuMP, as the name implies. Sometimes it may be required to refer to the specific documentation, as we'll present here only the documention specific to NLPModelsJuMP.\n\nPages = [\"tutorial.md\"]","category":"section"},{"location":"tutorial/#MathOptNLPModel","page":"Tutorial","title":"MathOptNLPModel","text":"MathOptNLPModel is a simple yet efficient model. It uses JuMP to define the problem, and can be accessed through the NLPModels API. An advantage of MathOptNLPModel over simpler models such as ADNLPModels is that they provide sparse derivates.\n\nLet's define the famous Rosenbrock function\n\nf(x) = (x_1 - 1)^2 + 100(x_2 - x_1^2)^2\n\nwith starting point x^0 = (-1210).\n\nusing NLPModels, NLPModelsJuMP, JuMP\n\nx0 = [-1.2; 1.0]\nmodel = Model() # No solver is required\n@variable(model, x[i=1:2], start=x0[i])\n@NLobjective(model, Min, (x[1] - 1)^2 + 100 * (x[2] - x[1]^2)^2)\n\nnlp = MathOptNLPModel(model)\n\nLet's get the objective function value at x^0, using only nlp.\n\nfx = obj(nlp, nlp.meta.x0)\nprintln(\"fx = $fx\")\n\nLet's try the gradient and Hessian.\n\ngx = grad(nlp, nlp.meta.x0)\nHx = hess(nlp, nlp.meta.x0)\nprintln(\"gx = $gx\")\nprintln(\"Hx = $Hx\")\n\nLet's do something a little more complex here, defining a function to try to solve this problem through steepest descent method with Armijo search. Namely, the method\n\nGiven x^0, varepsilon  0, and eta in (01). Set k = 0;\nIf Vert nabla f(x^k) Vert  varepsilon STOP with x^* = x^k;\nCompute d^k = -nabla f(x^k);\nCompute alpha_k in (01 such that f(x^k + alpha_kd^k)  f(x^k) + alpha_keta nabla f(x^k)^Td^k\nDefine x^k+1 = x^k + alpha_kx^k\nUpdate k = k + 1 and go to step 2.\n\nusing LinearAlgebra\n\nfunction steepest(nlp; itmax=100000, eta=1e-4, eps=1e-6, sigma=0.66)\n  x = nlp.meta.x0\n  fx = obj(nlp, x)\n  ∇fx = grad(nlp, x)\n  slope = dot(∇fx, ∇fx)\n  ∇f_norm = sqrt(slope)\n  iter = 0\n  while ∇f_norm > eps && iter < itmax\n    t = 1.0\n    x_trial = x - t * ∇fx\n    f_trial = obj(nlp, x_trial)\n    while f_trial > fx - eta * t * slope\n      t *= sigma\n      x_trial = x - t * ∇fx\n      f_trial = obj(nlp, x_trial)\n    end\n    x = x_trial\n    fx = f_trial\n    ∇fx = grad(nlp, x)\n    slope = dot(∇fx, ∇fx)\n    ∇f_norm = sqrt(slope)\n    iter += 1\n  end\n  optimal = ∇f_norm <= eps\n  return x, fx, ∇f_norm, optimal, iter\nend\n\nx, fx, ngx, optimal, iter = steepest(nlp)\nprintln(\"x = $x\")\nprintln(\"fx = $fx\")\nprintln(\"ngx = $ngx\")\nprintln(\"optimal = $optimal\")\nprintln(\"iter = $iter\")\n\nMaybe this code is too complicated? If you're in a class you just want to show a Newton step.\n\nf(x) = obj(nlp, x)\ng(x) = grad(nlp, x)\nH(x) = hess(nlp, x)\nx = nlp.meta.x0\nd = -H(x) \\ g(x)\n\nor a few\n\nfor i = 1:5\n  global x\n  x = x - H(x) \\ g(x)\n  println(\"x = $x\")\nend","category":"section"},{"location":"tutorial/#OptimizationProblems","page":"Tutorial","title":"OptimizationProblems","text":"The package OptimizationProblems provides a collection of problems defined in JuMP format, which can be converted to MathOptNLPModel.\n\nusing OptimizationProblems.PureJuMP  # Defines a lot of JuMP models\n\nnlp = MathOptNLPModel(woods())\nx, fx, ngx, optimal, iter = steepest(nlp)\nprintln(\"fx = $fx\")\nprintln(\"ngx = $ngx\")\nprintln(\"optimal = $optimal\")\nprintln(\"iter = $iter\")\n\nConstrained problems can also be converted.\n\nusing NLPModels, NLPModelsJuMP, JuMP\n\nmodel = Model()\nx0 = [-1.2; 1.0]\n@variable(model, x[i=1:2] >= 0.0, start=x0[i])\n@NLobjective(model, Min, (x[1] - 1)^2 + 100 * (x[2] - x[1]^2)^2)\n@constraint(model, x[1] + x[2] == 3.0)\n@NLconstraint(model, x[1] * x[2] >= 1.0)\n\nnlp = MathOptNLPModel(model)\n\nprintln(\"cx = $(cons(nlp, nlp.meta.x0))\")\nprintln(\"Jx = $(jac(nlp, nlp.meta.x0))\")","category":"section"},{"location":"tutorial/#MathOptNLSModel","page":"Tutorial","title":"MathOptNLSModel","text":"MathOptNLSModel is a model for nonlinear least squares using JuMP, The objective function of NLS problems has the form f(x) = tfrac12F(x)^2, but specialized methods handle F directly, instead of f. To use MathOptNLSModel, we define a JuMP model without the objective, and use NLexpressions to define the residual function F. For instance, the Rosenbrock function can be expressed in nonlinear least squares format by defining\n\nF(x) = beginbmatrix x_1 - 1 10(x_2 - x_1^2) endbmatrix\n\nand noting that f(x) = F(x)^2 (the constant frac12 is ignored as it doesn't change the solution). We implement this function as\n\nusing NLPModels, NLPModelsJuMP, JuMP\n\nmodel = Model()\nx0 = [-1.2; 1.0]\n@variable(model, x[i=1:2], start=x0[i])\n@NLexpression(model, F1, x[1] - 1)\n@NLexpression(model, F2, 10 * (x[2] - x[1]^2))\n\nnls = MathOptNLSModel(model, [F1, F2], name=\"rosen-nls\")\n\nresidual(nls, nls.meta.x0)\n\njac_residual(nls, nls.meta.x0)","category":"section"},{"location":"tutorial/#NLSProblems","page":"Tutorial","title":"NLSProblems","text":"The package NLSProblems provides a collection of problems already defined as MathOptNLSModel.","category":"section"},{"location":"tutorial/#NLPModelsJuMP.MathOptNLPModel","page":"Tutorial","title":"NLPModelsJuMP.MathOptNLPModel","text":"MathOptNLPModel(model, hessian=true, name=\"Generic\")\n\nConstruct a MathOptNLPModel from a JuMP model.\n\nhessian should be set to false for multivariate user-defined functions registered without hessian.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/#NLPModelsJuMP.MathOptNLSModel","page":"Tutorial","title":"NLPModelsJuMP.MathOptNLSModel","text":"MathOptNLSModel(model, F, hessian=true, name=\"Generic\")\n\nConstruct a MathOptNLSModel from a JuMP model and a container of JuMP GenericAffExpr (generated by @expression) and NonlinearExpression (generated by @NLexpression).\n\nhessian should be set to false for multivariate user-defined functions registered without hessian.\n\n\n\n\n\n","category":"type"}]
}
